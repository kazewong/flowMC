{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing sampling strategy\n",
    "\n",
    "The default strategy of flowMC looks like the following:\n",
    "\n",
    "1. In training phase\n",
    "    1. Sample with local sampler\n",
    "    2. Train normalizing flow with the samples\n",
    "    3. Sample with global sampler\n",
    "2. Freeze the normalizing flow\n",
    "3. In production phase\n",
    "    1. Sample with local sampler\n",
    "    2. Sample with global sampler\n",
    "\n",
    "\n",
    "One of the major changes introduced in `flowMC` 0.4.0 is the ability to customize the sampling strategy. In this notebook, we will demonstrate how to add an optimization step before the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "from flowMC.resource.nf_model.rqSpline import MaskedCouplingRQSpline\n",
    "from flowMC.resource.local_kernel.MALA import MALA\n",
    "from flowMC.Sampler import Sampler\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from flowMC.strategy.optimization import optimization_Adam\n",
    "\n",
    "n_dim = 5\n",
    "\n",
    "\n",
    "def target_dual_moon(x, data=None):\n",
    "    \"\"\"\n",
    "    Term 2 and 3 separate the distribution and smear it along the first and second dimension\n",
    "    \"\"\"\n",
    "    term1 = 0.5 * ((jnp.linalg.norm(x) - 2) / 0.1) ** 2\n",
    "    term2 = -0.5 * ((x[:1] + jnp.array([-3.0, 3.0])) / 0.8) ** 2\n",
    "    term3 = -0.5 * ((x[1:2] + jnp.array([-3.0, 3.0])) / 0.6) ** 2\n",
    "    return -(term1 - logsumexp(term2) - logsumexp(term3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let say our initialization is way off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chains = 20\n",
    "\n",
    "rng_key, subkey = jax.random.split(jax.random.PRNGKey(42))\n",
    "# Instead of initializing with a unit gaussian, we initialize with a gaussian with a larger variance\n",
    "initial_position = jax.random.normal(subkey, shape=(n_chains, n_dim)) * 100\n",
    "\n",
    "n_dim = 5\n",
    "n_layers = 4\n",
    "hidden_size = [32, 32]\n",
    "num_bins = 8\n",
    "data = jnp.zeros(n_dim)\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "model = MaskedCouplingRQSpline(n_dim, n_layers, hidden_size, num_bins, subkey)\n",
    "MALA_Sampler = MALA(target_dual_moon, True, step_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_loop_training = 20\n",
    "n_loop_production = 20\n",
    "n_local_steps = 100\n",
    "n_global_steps = 10\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "batch_size = 5000\n",
    "max_samples = 5000\n",
    "\n",
    "\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nf_sampler = Sampler(\n",
    "    n_dim,\n",
    "    subkey,\n",
    "    {\"data\": data},\n",
    "    MALA_Sampler,\n",
    "    model,\n",
    "    n_loop_training=n_loop_training,\n",
    "    n_loop_production=n_loop_production,\n",
    "    n_local_steps=n_local_steps,\n",
    "    n_global_steps=n_global_steps,\n",
    "    n_chains=n_chains,\n",
    "    n_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    momentum=momentum,\n",
    "    batch_size=batch_size,\n",
    "    use_global=True,\n",
    ")\n",
    "print(nf_sampler.strategies)\n",
    "nf_sampler.sample(initial_position, data={\"data\": data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the chain need to started off really far from the most probable set. This is not a huge problem for this example since the posterior is rather simple and MALA uses gradient in its proposal. Still, one can see there is a huge jump in the NF loss at some point in the during, basically because the distribution the flow is approximating changes a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = nf_sampler.get_sampler_state(training=True)\n",
    "chains = np.array(out_train[\"chains\"])\n",
    "global_accs = np.array(out_train[\"global_accs\"])\n",
    "local_accs = np.array(out_train[\"local_accs\"])\n",
    "loss_vals = np.array(out_train[\"loss_vals\"])\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nf_samples = np.array(nf_sampler.sample_flow(subkey, 3000))\n",
    "\n",
    "\n",
    "# Plot 2 chains in the plane of 2 coordinates for first visual check\n",
    "plt.figure(figsize=(6, 6))\n",
    "axs = [plt.subplot(2, 2, i + 1) for i in range(4)]\n",
    "plt.sca(axs[0])\n",
    "plt.title(\"2d proj of 2 chains\")\n",
    "\n",
    "plt.plot(chains[0, :, 0], chains[0, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.plot(chains[1, :, 0], chains[1, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.title(\"NF loss\")\n",
    "plt.plot(loss_vals.reshape(-1))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[2])\n",
    "plt.title(\"Local Acceptance\")\n",
    "plt.plot(local_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[3])\n",
    "plt.title(\"Global Acceptance\")\n",
    "plt.plot(global_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.tight_layout()\n",
    "plt.show(block=False)\n",
    "\n",
    "labels = [\"$x_1$\", \"$x_2$\", \"$x_3$\", \"$x_4$\", \"$x_5$\"]\n",
    "# Plot all chains\n",
    "figure = corner.corner(chains.reshape(-1, n_dim), labels=labels)\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.suptitle(\"Visualize samples\")\n",
    "plt.show(block=False)\n",
    "\n",
    "# Plot Nf samples\n",
    "figure = corner.corner(nf_samples, labels=labels)\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.suptitle(\"Visualize NF samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to run the same example but with an extra step in the sampling strategy: we will run Adam some number of steps before starting the normalizing flow training. This should help the normalizing flow to start closer to the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_loop_training = 20\n",
    "n_loop_production = 20\n",
    "n_local_steps = 100\n",
    "n_global_steps = 10\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "batch_size = 5000\n",
    "max_samples = 5000\n",
    "\n",
    "Adam_opt = optimization_Adam(n_steps=10000, learning_rate=1, noise_level=1)\n",
    "\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nf_sampler = Sampler(\n",
    "    n_dim,\n",
    "    subkey,\n",
    "    {\"data\": data},\n",
    "    MALA_Sampler,\n",
    "    model,\n",
    "    n_loop_training=n_loop_training,\n",
    "    n_loop_production=n_loop_production,\n",
    "    n_local_steps=n_local_steps,\n",
    "    n_global_steps=n_global_steps,\n",
    "    n_chains=n_chains,\n",
    "    n_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    momentum=momentum,\n",
    "    batch_size=batch_size,\n",
    "    use_global=True,\n",
    "    strategies=[Adam_opt, \"default\"],\n",
    ")\n",
    "print(nf_sampler.strategies)\n",
    "nf_sampler.sample(initial_position, data={\"data\": data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the chains are much closer to the target distribution from the start, hence the normalizing flow training is much smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = nf_sampler.get_sampler_state(training=True)\n",
    "chains = np.array(out_train[\"chains\"])\n",
    "global_accs = np.array(out_train[\"global_accs\"])\n",
    "local_accs = np.array(out_train[\"local_accs\"])\n",
    "loss_vals = np.array(out_train[\"loss_vals\"])\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nf_samples = np.array(nf_sampler.sample_flow(subkey, 3000))\n",
    "\n",
    "\n",
    "# Plot 2 chains in the plane of 2 coordinates for first visual check\n",
    "plt.figure(figsize=(6, 6))\n",
    "axs = [plt.subplot(2, 2, i + 1) for i in range(4)]\n",
    "plt.sca(axs[0])\n",
    "plt.title(\"2d proj of 2 chains\")\n",
    "\n",
    "plt.plot(chains[0, :, 0], chains[0, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.plot(chains[1, :, 0], chains[1, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.title(\"NF loss\")\n",
    "plt.plot(loss_vals.reshape(-1))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[2])\n",
    "plt.title(\"Local Acceptance\")\n",
    "plt.plot(local_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[3])\n",
    "plt.title(\"Global Acceptance\")\n",
    "plt.plot(global_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.tight_layout()\n",
    "plt.show(block=False)\n",
    "\n",
    "labels = [\"$x_1$\", \"$x_2$\", \"$x_3$\", \"$x_4$\", \"$x_5$\"]\n",
    "# Plot all chains\n",
    "figure = corner.corner(chains.reshape(-1, n_dim), labels=labels)\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.suptitle(\"Visualize samples\")\n",
    "plt.show(block=False)\n",
    "\n",
    "# Plot Nf samples\n",
    "figure = corner.corner(nf_samples, labels=labels)\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.suptitle(\"Visualize NF samples\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
